---
title: "WY_xgboost"
author: "Shannon Curley"
date: "2024-11-20"
output: html_document
---

```{r}
library(xgboost) # xgboost models
library(dplyr) # df manipulation 
library(caret) # model evaluation
library(ggplot2) # plotting
library(patchwork) # plot layout
library(data.table) # to work with xgboost data format
library(tidyr)

####################################################################
### Code to train WY composite data to eBird grid for state-wide map
####################################################################


wy_grid <- read.csv("gis/wy_grid.csv")
wyoming_raster <- rast('gis/srd_3km_mask_land.tif')

### radar means data
vid_cell_week_hour <- read.csv("gis/new_vid_cell_week_hour.csv")

### adjacency matrix (8-direction)
adj_matrix <- adjacent(wyoming_raster, cells = wy_grid$srd_id, 
                       directions = 8, pairs = TRUE)
adj_df <- as.data.frame(adj_matrix)

df_neighbors <- vid_cell_week_hour %>%
  left_join(
    adj_df,
    by = c("cell" = "from"),
    relationship = "many-to-many"
  ) %>%
  left_join(
    vid_cell_week_hour %>% select(cell, week, hour, season, mean),
    by = c("to" = "cell", "week", "hour", "season"),
    suffix = c("", "_neighbor"),
    relationship = "many-to-many"
  ) %>%
  group_by(cell, week, hour, season) %>%
  summarise(mean = first(mean), mean_adj = mean(mean_neighbor, na.rm = TRUE), .groups = "drop")


### rename column to cell
colnames(wy_grid)[1]<-c("cell")

### join the data 
WY_XG<-dplyr::left_join(df_neighbors, wy_grid, by = "cell") #1,420,573 obs with 269 variables

### remove cell_on_land, and year (since we only have 2023)
WY_XG<-dplyr::select(WY_XG,-cell_on_land, year)

WY_XG <- WY_XG %>%
  dplyr::select(-x, -y, -longitude, -latitude, -cell)

### Divide data into training set (70%) to train model, validation set (20%) for model tuning, and test set (10%) for evaluation 

### to reproduce
set.seed(123)  

### shuffle data (default is sample wihout replacement)
shuffled_data <- WY_XG %>% sample_frac(1, replace = FALSE)

### Get number of rows of the data
n <- nrow(shuffled_data)

### estimates 70% of data for training set
train_index <- floor(0.7 * n)

### 
val_index <- floor(0.9 * n)

### split the data
training_set <- shuffled_data[1:train_index, ]
validation_set <- shuffled_data[(train_index + 1):val_index, ]
test_set <- shuffled_data[(val_index + 1):n, ]

### Convert to XGBoost matrix
# training set
dtrain <- xgb.DMatrix(
  data = data.matrix(training_set %>% dplyr::select(-mean)), 
  label = training_set$mean
)

# validation set 
dvalid <- xgb.DMatrix(
  data = data.matrix(validation_set %>% dplyr::select(-mean)), 
  label = validation_set$mean
)

# test set
dtest <- xgb.DMatrix(
  data = data.matrix(test_set %>% dplyr::select(-mean)), 
  label = test_set$mean
)

### evals for model training (replaces "watchlist" in earlier versions of xgboost)
evals <- list(train = dtrain, test = dvalid)

### Train the model with xgboost default parameters

WY_model <- xgb.train(
    params = list(
    booster = "gbtree",           
    objective = "reg:squarederror",
    alpha = 0.1),
    data = dtrain,
    nrounds = 10000,   
    nthread = 10,
    evals = evals,              
    early_stopping_rounds = 10,      
    verbose = 1                      
)


### save feature names from the training set since by default xgboost doesn't save
feature_names <- colnames(training_set %>% dplyr::select(-mean))
saveRDS(feature_names, "gis/feature_names.rds")


### save the model
#xgb.save(WY_model, "~/Desktop/WY_XGBoost/WY_model.xgb")


### load the model (if not already in environment)
WY_model <- xgb.load("~/Desktop/WY_XGBoost/WY_model.xgb")

### load the feature names
feature_names <- readRDS("~/Desktop/WY_XGBoost/feature_names.rds")

### predicts the mean onto the test dataset for evaluation
predictions <- predict(WY_model, newdata = dtest)

### get the mean from the test set
actual_mean <- test_set$mean

### calculate RMSE (which you can also grab from the model)
rmse <- sqrt(mean((predictions - actual_mean)^2))
ss_total <- sum((actual_mean - mean(actual_mean))^2)
### residuals
ss_residual <- sum((actual_mean - predictions)^2)
### r2
r_squared <- 1 - (ss_residual / ss_total) #0.94

### Attach predictions to test set
test_set$predictions <- predictions

### convert to df and map the features back to model
importance_matrix <- as.data.frame(xgb.importance(model = WY_model))

### This part is really annoying
feature_mapping <- setNames(feature_names, paste0("f", seq_along(feature_names) - 1))
importance_matrix$Feature <- feature_mapping[importance_matrix$Feature]


### Plot top 10 features by Gain
WY_a <- ggplot(importance_matrix[1:10, ], aes(x = reorder(Feature, Gain), y = Gain)) +
  geom_bar(stat = "identity", fill = "grey40") +
  coord_flip() +
  labs(title = "a)", x = "Features", y = "Gain") +
  theme_classic()


### Select top 6 features (can select more but the plot gets messy)
top_features <- importance_matrix$Feature[1:6]

### Subsample test data otherwise takes too long to plot
set.seed(42) ### to reproduce
WY_data_to_plot <- test_set %>%
  dplyr::select(all_of(top_features), predictions) %>%
  sample_n(5000)

### Reshape for ggplot
WY_plot_data <- WY_data_to_plot %>%
  pivot_longer(cols = all_of(top_features), names_to = "Feature", values_to = "Value") %>%
  mutate(Feature = factor(Feature, levels = top_features))  

### relationship mean~feature vales
WY_b <- ggplot(WY_plot_data, aes(x = Value, y = predictions)) +
  geom_smooth(method = "loess", se = FALSE, color = "grey40") +
  facet_wrap(~ Feature, ncol = 3, scales = "free_x") +
  labs(title = "b)", x = "Feature Value", y = "Predicted Mean Density") +
  theme_classic()

### arrange plot
WY_combo <- WY_a + WY_b + plot_layout(widths = c(1, 3))

# Create a data frame of your metrics
evaluation_metrics <- data.frame(
  Metric = c("RMSE", "R-squared"),
  Value = c(rmse, r_squared)
)

# Using knitr::kable for a simple static table
knitr::kable(evaluation_metrics, caption = "Model Evaluation Metrics")

# Or use DT::datatable for an interactive table
library(DT)
datatable(evaluation_metrics, caption = "Model Evaluation Metrics")



weeks_to_predict <- c(19, 20, 21, 22, 36, 37, 38, 39)
hour_to_predict <- 3

# Create a full grid of cell-week-hour combinations
all_combinations <- expand_grid(
  cell = wy_grid$cell,
  week = weeks_to_predict,
  hour = hour_to_predict
)

all_combinations <- all_combinations %>%
  anti_join(
    vid_cell_week_hour %>% distinct(cell), 
    by = "cell"
  )

prediction_data <- left_join(all_combinations, wy_grid, by = "cell")

prediction_data <- prediction_data %>% 
  dplyr::select(-longitude, -latitude, -cell_on_land, -year, -cell)


dpred <- xgb.DMatrix(data = data.matrix(prediction_data))
pred_values <- predict(WY_model, newdata = dpred)
final_predictions <- bind_cols(all_combinations, predictions = pred_values)




```

